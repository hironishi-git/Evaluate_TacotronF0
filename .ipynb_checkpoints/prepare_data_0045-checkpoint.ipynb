{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "from os.path import basename, join\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "from scipy.io import wavfile,loadmat\n",
    "from scipy.io.wavfile import read\n",
    "from glob import glob\n",
    "from os.path import join,basename\n",
    "from pydub import AudioSegment\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nnmnkwii import metrics\n",
    "\n",
    "import layers\n",
    "from scipy.spatial.distance  import euclidean\n",
    "from scipy import signal,stats,fftpack\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wav_to_torch(path):\n",
    "    sampling_rate, data = read(path)\n",
    "    return torch.FloatTensor(data.astype(np.float32)), sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_melspec(data):\n",
    "    audio_norm = data/32768\n",
    "    audio_norm = audio_norm.unsqueeze(0)\n",
    "    audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
    "    stft=layers.TacotronSTFT(1024, 200, 800,80, 16000, 0.0,8000.0)\n",
    "    melspec = stft.mel_spectrogram(audio_norm)\n",
    "    melspec = torch.squeeze(melspec, 0)\n",
    "    return melspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spec(mel):\n",
    "    stft = layers.TacotronSTFT(1024,200,800, sampling_rate=16000)\n",
    "    mel_decompress = stft.spectral_de_normalize(mel)\n",
    "    mel_decompress = mel_decompress.transpose(1, 2).data.cpu()\n",
    "    mel_decompress = mel_decompress.float()\n",
    "    spec_from_mel_scaling = 1000\n",
    "    spec_from_mel = torch.mm(mel_decompress[0], stft.mel_basis)\n",
    "    spec_from_mel = spec_from_mel.transpose(0, 1).unsqueeze(0)\n",
    "    spec_from_mel = spec_from_mel * spec_from_mel_scaling\n",
    "    return spec_from_mel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filename, split=\"|\"):\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        dataset = [line.strip().split(split) for line in f]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_speaker_lookup_table(dataset):\n",
    "        speaker_ids = np.sort(np.unique([x[2] for x in dataset]))\n",
    "        d = {int(speaker_ids[i]): i for i in range(len(speaker_ids))}\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path =  '/home/drobo/hironishi/work/research/data_manage/tacotron2_dataset/csj_dataset/txts/002/train.txt'\n",
    "dataset = load_dataset(train_path)\n",
    "#Speaker identification for input to the model\n",
    "ISMS = create_speaker_lookup_table(dataset) # Input Speaker_id :  Speaker_id in Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = './DATA/005'\n",
    "os.makedirs(base_dir,exist_ok=True)\n",
    "#標本化周波数\n",
    "fs =16000\n",
    "conpute = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grand_truth_dir = join(base_dir,'Grand_truth')\n",
    "os.makedirs(Grand_truth_dir,exist_ok=True)\n",
    "tool_dir = join(base_dir,'tool')\n",
    "os.makedirs(tool_dir,exist_ok=True)\n",
    "SSI_path = join(tool_dir,'SSI.pickle')\n",
    "synth_text_path = join(tool_dir,'exp_text.txt')\n",
    "source_list_path = join(tool_dir,'source_list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path ='/home/drobo/hironishi/work/research/data_manage/tacotron2_dataset/csj_dataset/txts/002/test.txt'\n",
    "with open(test_path,'r') as f:\n",
    "    # Audio path:Text:Speaker_id\n",
    "    FATS = {} \n",
    "    DATS = {}\n",
    "    for line in f:\n",
    "        audio_path,text,speaker = line.split('|')\n",
    "        speaker=int(speaker.replace('\\n','')) \n",
    "        #発話の長さが10字以上のもの   \n",
    "        if len(text)>=10 and audio_path.split('/')[-3]=='WAV':\n",
    "            FATS[audio_path]={}\n",
    "            FATS[audio_path][text]=speaker\n",
    "        if len(text)>=10 and audio_path.split('/')[-3]=='WAV_df_fps':\n",
    "            DATS[audio_path]={}\n",
    "            DATS[audio_path][text]=speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAkeys = random.sample(list(FATS.keys()),100)\n",
    "DAkeys = random.sample(list(DATS.keys()),100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [01:51,  1.11s/it]\n"
     ]
    }
   ],
   "source": [
    "if conpute:\n",
    "    with open(SSI_path,mode ='wb')as out:\n",
    "        SSI_converted = [ ISMS[list(DATS[audio].values())[0]]  for audio in DAkeys ]\n",
    "        pickle.dump(SSI_converted,out)\n",
    "        out.close()\n",
    "    \n",
    "    for i, file in enumerate(DAkeys):\n",
    "            %cp $file $Grand_truth_dir\n",
    "            f_name = os.path.basename(file)\n",
    "            os.rename(f\"{Grand_truth_dir+'/'}{f_name}\", f\"{Grand_truth_dir+'/'}{'text_'+str(i).zfill(3)}.wav\")\n",
    "    \n",
    "    with open(synth_text_path,'w') as syn:\n",
    "        with open(source_list_path,'w') as w:\n",
    "            for path in DAkeys:\n",
    "                text = list(DATS[path].keys())[0]+'\\n'\n",
    "                path = path+'\\n'\n",
    "                syn.write(text)\n",
    "                w.write(path) \n",
    "    \n",
    "    from audio_processing import griffin_lim\n",
    "    from scipy.io.wavfile  import write\n",
    "    with open(join(base_dir,'tool/source_list'),'r')as f:\n",
    "        wavpaths =  [path.replace('\\n','')  for path in f]\n",
    "        \n",
    "    gt_mels = []  \n",
    "    for wavpath in wavpaths:\n",
    "        data,sr = load_wav_to_torch(wavpath)\n",
    "        mel_spec = compute_melspec(data)\n",
    "        gt_mels.append(mel_spec)\n",
    "        \n",
    "        \n",
    "    out_dir = join(base_dir,'Grand_truth_griffin_lim')\n",
    "    os.makedirs(out_dir,exist_ok=True)\n",
    "    stft = layers.TacotronSTFT(1024,200,800, sampling_rate=16000)\n",
    "    for i, mel in tqdm(enumerate(gt_mels)):\n",
    "        spec = get_spec(mel.unsqueeze(0))\n",
    "        audio = griffin_lim(torch.autograd.Variable(spec[:, :, :-1]), stft.stft_fn, 100)\n",
    "        audio = audio.squeeze()\n",
    "        audio = audio.data.cpu().numpy()\n",
    "        #audio = audio/np.max(np.abs(audio))\n",
    "        write(join(out_dir,'text_{}.wav'.format(str(i).zfill(3))),16000,audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
