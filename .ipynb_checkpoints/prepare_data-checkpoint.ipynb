{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 評価音声を決める\n",
    "ここでは以下の条件を満たす評価音声を決める\\\n",
    "話者id 男性5名,女性5名\\\n",
    "各話者の発話は流暢な発話×10の100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "from os.path import basename, join\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "from scipy.io import wavfile,loadmat\n",
    "from scipy.io.wavfile import read\n",
    "from glob import glob\n",
    "from os.path import join,basename\n",
    "from pydub import AudioSegment\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nnmnkwii import metrics\n",
    "\n",
    "import layers\n",
    "from  fastdtw  import fastdtw\n",
    "from scipy.spatial.distance  import euclidean\n",
    "from scipy import signal,stats,fftpack\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wav_to_torch(path):\n",
    "    sampling_rate, data = read(path)\n",
    "    return torch.FloatTensor(data.astype(np.float32)), sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_melspec(data):\n",
    "    audio_norm = data/32768\n",
    "    audio_norm = audio_norm.unsqueeze(0)\n",
    "    audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
    "    stft=layers.TacotronSTFT(1024, 200, 800,80, 16000, 0.0,8000.0)\n",
    "    melspec = stft.mel_spectrogram(audio_norm)\n",
    "    melspec = torch.squeeze(melspec, 0)\n",
    "    return melspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spec(mel):\n",
    "    stft = layers.TacotronSTFT(1024,200,800, sampling_rate=16000)\n",
    "    mel_decompress = stft.spectral_de_normalize(mel)\n",
    "    mel_decompress = mel_decompress.transpose(1, 2).data.cpu()\n",
    "    mel_decompress = mel_decompress.float()\n",
    "    spec_from_mel_scaling = 1000\n",
    "    spec_from_mel = torch.mm(mel_decompress[0], stft.mel_basis)\n",
    "    spec_from_mel = spec_from_mel.transpose(0, 1).unsqueeze(0)\n",
    "    spec_from_mel = spec_from_mel * spec_from_mel_scaling\n",
    "    return spec_from_mel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = './DATA/004'\n",
    "#標本化周波数\n",
    "fs =16000\n",
    "conpute = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 話者の選定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでは、学習に使用した講演話者IDおよび、学習に使用していないテキストと音声ファイルを取得する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path ='/home/drobo/hironishi/work/research/data_manage/tacotron2_dataset/csj_dataset/txts/002/test.txt'\n",
    "with open(test_path,'r') as f:\n",
    "    SAT = {}  # input Speaker_id : {Audio path:Text}\n",
    "    for line in f:\n",
    "        audio_path,text,speaker = line.split('|')\n",
    "        speaker=int(speaker.replace('\\n',''))\n",
    "        if speaker  not in  SAT.keys():\n",
    "            SAT[speaker]={}\n",
    "            SAT[speaker][audio_path]=text\n",
    "        else:\n",
    "            SAT[speaker][audio_path]=text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 講演話者IDと、モデルに実際に入力する話者IDの辞書を作成する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filename, split=\"|\"):\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        dataset = [line.strip().split(split) for line in f]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_speaker_lookup_table(dataset):\n",
    "        speaker_ids = np.sort(np.unique([x[2] for x in dataset]))\n",
    "        d = {int(speaker_ids[i]): i for i in range(len(speaker_ids))}\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path =  '/home/drobo/hironishi/work/research/data_manage/tacotron2_dataset/csj_dataset/txts/002/train.txt'\n",
    "dataset = load_dataset(train_path)\n",
    "#Speaker identification for input to the model\n",
    "ISMS = create_speaker_lookup_table(dataset) # Input Speaker_id :  Speaker_id in Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_speakers=[]\n",
    "F_speakers =[]\n",
    "for speaker_id,dic in SAT.items():\n",
    "    wav_paths = [audio_path for audio_path in dic.keys() if audio_path.split('/')[-3]=='WAV']\n",
    "    #テキストの長さが10字以上のもの&流暢なもの\n",
    "    texts = [text for  audio,text in dic.items() if audio.split('/')[-3]=='WAV' and len(text)>=10]\n",
    "    if len(wav_paths) > 0  and len(texts)>10:\n",
    "        if list(SAT[speaker_id].keys())[0].split('/')[-2][3] == 'M':\n",
    "            M_speakers.append(speaker_id) \n",
    "        else:\n",
    "            F_speakers.append(speaker_id)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 話者を決める\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでは男女各5名をランダムに選ぶ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1391, 1247, 1331, 1137, 1193, 574, 1056, 791, 804, 449, 122, 1304, 911, 255, 1309, 956, 771, 863, 1443, 1409, 921, 337, 732, 1016, 381, 1197, 947, 893, 577, 1039, 803] [659, 825, 453, 1322, 1113, 1256, 698, 995, 461, 1337, 1390, 1104, 452, 988, 814, 1367, 192, 544, 1350, 1338, 842, 603, 1105, 918, 1222, 148, 849, 564, 1098, 68, 978, 605, 1077, 807, 951, 1274, 824, 264, 630, 935, 1043, 639, 1170, 1307, 523, 604, 1182]\n"
     ]
    }
   ],
   "source": [
    "print(M_speakers,F_speakers )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers = random.sample(M_speakers,2)\n",
    "speakers[len(speakers):len(speakers)] =  random.sample(F_speakers,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 発話内容を決める"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choice speaker:audio:text\n",
    "CSAT={}\n",
    "for sp in speakers:\n",
    "    CSAT[sp]={}\n",
    "    i =0\n",
    "    for  audio in SAT[sp]:\n",
    "        text = SAT[sp][audio]\n",
    "        if audio.split('/')[-3]=='WAV' and  len(text)>=10 and i<10:\n",
    "            i+=1\n",
    "            CSAT[sp][audio]= text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling input Speaker_id list\n",
    "SSI_converted = [] \n",
    "# natural audio path\n",
    "NAP = []\n",
    "for sp, audio_and_text in  CSAT.items():\n",
    "     for audio,text in audio_and_text.items():\n",
    "            SSI_converted.append(ISMS[sp])\n",
    "            NAP.append(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "make = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/home/drobo/hironishi/work/research/Evalution/QE_tacotronF0/DATA/004'\n",
    "os.makedirs(base_dir,exist_ok=True)\n",
    "Grand_truth_dir = join(base_dir,'Grand_truth')\n",
    "os.makedirs(Grand_truth_dir,exist_ok=True)\n",
    "tool_dir = join(base_dir,'tool')\n",
    "os.makedirs(tool_dir,exist_ok=True)\n",
    "\n",
    "SSI_path = join(tool_dir,'SSI.pickle')\n",
    "synth_text_path = join(tool_dir,'exp_text.txt')\n",
    "source_list_path = join(tool_dir,'source_list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "135it [00:00, 65848.48it/s]\n"
     ]
    }
   ],
   "source": [
    "test_path = '/home/drobo/hironishi/work/research/Evalution/QE_tacotronF0/DATA/004/tool/filled_pose.txt'\n",
    "SSI_converted = [] #話者id \n",
    "NAP = [] #自然音声\n",
    "texts = [] \n",
    "with open(test_path,'r')as f:\n",
    "    for line in tqdm(f): \n",
    "        audio_path,text,speaker = line.split('|')\n",
    "        speaker=int(speaker.replace('\\n',''))\n",
    "        SSI_converted.append(ISMS[speaker])\n",
    "        NAP.append(audio_path)\n",
    "        texts.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "if make:\n",
    "    with open(SSI_path,mode ='wb')as out:\n",
    "        pickle.dump(SSI_converted,out)\n",
    "        out.close()\n",
    "    \n",
    "    for i, file in enumerate(NAP):\n",
    "            %cp $file $Grand_truth_dir\n",
    "            f_name = os.path.basename(file)\n",
    "            os.rename(f\"{Grand_truth_dir+'/'}{f_name}\", f\"{Grand_truth_dir+'/'}{'text_'+str(i).zfill(3)}.wav\")\n",
    "    \n",
    "    with open(synth_text_path,'w') as syn:\n",
    "        with open(source_list_path,'w') as w:\n",
    "            for path,text in zip(NAP,texts):\n",
    "                text =  text+'\\n'\n",
    "                path = path+'\\n'\n",
    "                syn.write(text)\n",
    "                w.write(path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if make:\n",
    "    with open(SSI_path,mode ='wb')as out:\n",
    "        pickle.dump(SSI_converted,out)\n",
    "        out.close()\n",
    "    \n",
    "    for i, file in enumerate(NAP):\n",
    "            %cp $file $Grand_truth_dir\n",
    "            f_name = os.path.basename(file)\n",
    "            os.rename(f\"{Grand_truth_dir+'/'}{f_name}\", f\"{Grand_truth_dir+'/'}{'text_'+str(i).zfill(3)}.wav\")\n",
    "    \n",
    "    with open(synth_text_path,'w') as syn:\n",
    "        with open(source_list_path,'w') as w:\n",
    "            for dic in CSAT.values():\n",
    "                for path,text  in dic.items():\n",
    "                    text =  text+'\\n'\n",
    "                    path = path+'\\n'\n",
    "                    syn.write(text)\n",
    "                    w.write(path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "135it [02:34,  1.15s/it]\n"
     ]
    }
   ],
   "source": [
    "if conpute :\n",
    "    from audio_processing import griffin_lim\n",
    "    from scipy.io.wavfile  import write\n",
    "    with open(join(base_dir,'tool/source_list'),'r')as f:\n",
    "        wavpaths =  [path.replace('\\n','')  for path in f]\n",
    "        \n",
    "    gt_mels = []  \n",
    "    for wavpath in wavpaths:\n",
    "        data,sr = load_wav_to_torch(wavpath)\n",
    "        mel_spec = compute_melspec(data)\n",
    "        gt_mels.append(mel_spec)\n",
    "        \n",
    "        \n",
    "    out_dir = join(base_dir,'Grand_truth_griffin_lim')\n",
    "    os.makedirs(out_dir,exist_ok=True)\n",
    "    stft = layers.TacotronSTFT(1024,200,800, sampling_rate=16000)\n",
    "    for i, mel in tqdm(enumerate(gt_mels)):\n",
    "        spec = get_spec(mel.unsqueeze(0))\n",
    "        audio = griffin_lim(torch.autograd.Variable(spec[:, :, :-1]), stft.stft_fn, 100)\n",
    "        audio = audio.squeeze()\n",
    "        audio = audio.data.cpu().numpy()\n",
    "        audio = audio/np.max(np.abs(audio))\n",
    "        write(join(out_dir,'text_{}.wav'.format(str(i).zfill(3))),16000,audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
