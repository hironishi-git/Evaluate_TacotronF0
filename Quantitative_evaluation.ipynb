{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TacotronF0の定量評価\n",
    "\n",
    "ここではTacotronF0の定量評価を行う。具体的にはF0パターンを調査する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "\n",
    "from scipy.io import wavfile,loadmat\n",
    "from scipy.io.wavfile import read\n",
    "from glob import glob\n",
    "from os.path import join,basename\n",
    "from pydub import AudioSegment\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nnmnkwii import metrics\n",
    "\n",
    "import layers\n",
    "from  fastdtw  import fastdtw\n",
    "from scipy.spatial.distance  import euclidean\n",
    "from scipy import signal,stats,fftpack\n",
    "import statistics\n",
    "\n",
    "import IPython.display as ipd\n",
    "from IPython.display import Audio,display,Image,display_png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(data,figsize=(16, 4)):\n",
    "    fig, axes = plt.subplots(1, len(data), figsize=figsize)\n",
    "    for i in range(len(data)):\n",
    "        axes[i].imshow(data[i], aspect='auto', origin='bottom', \n",
    "                       interpolation='none')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lf0_vuv(path):\n",
    "    with open(path,mode='rb') as f:\n",
    "        lf0,vuv = pickle.load(f).values()\n",
    "        return lf0,vuv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wav_to_torch(path):\n",
    "    sampling_rate, data = read(path)\n",
    "    return torch.FloatTensor(data.astype(np.float32)), sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_melspec(data):\n",
    "    audio_norm = data/32768\n",
    "    audio_norm = audio_norm.unsqueeze(0)\n",
    "    audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
    "    stft=layers.TacotronSTFT(1024, 200, 800,80, 16000, 0.0,8000.0)\n",
    "    melspec = stft.mel_spectrogram(audio_norm)\n",
    "    melspec = torch.squeeze(melspec, 0)\n",
    "    return melspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spec(mel):\n",
    "    stft = layers.TacotronSTFT(1024,200,800, sampling_rate=16000)\n",
    "    mel_decompress = stft.spectral_de_normalize(mel)\n",
    "    mel_decompress = mel_decompress.transpose(1, 2).data.cpu()\n",
    "    mel_decompress = mel_decompress.float()\n",
    "    spec_from_mel_scaling = 1000\n",
    "    spec_from_mel = torch.mm(mel_decompress[0], stft.mel_basis)\n",
    "    spec_from_mel = spec_from_mel.transpose(0, 1).unsqueeze(0)\n",
    "    spec_from_mel = spec_from_mel * spec_from_mel_scaling\n",
    "    return spec_from_mel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conpute_f0_from_spec(spec):\n",
    "    _spec = spec[0].transpose(0,1)\n",
    "    f0_from_ceps = []\n",
    "    ceps = []\n",
    "    peaks=[]\n",
    "    for sp in _spec:\n",
    "        sp[sp==0.0]= 1e-3\n",
    "        log_sp = torch.log(torch.abs(sp)**2)\n",
    "        cep = np.fft.ifft(log_sp)\n",
    "        cep = cep[0:int(len(cep)/2)]\n",
    "        ceps.append(cep)\n",
    "        # f0の範囲が80hz-400hzになるように時間範囲を指定\n",
    "        index_range = [ i  for i,x in enumerate(range(81),20)]\n",
    "        peak = max(cep[index_range])\n",
    "        peak_index = np.where(cep==peak)[0]\n",
    "        f0 = 8000/peak_index\n",
    "        f0_from_ceps.append(f0)\n",
    "        f0 = [ f[0] for f in f0_from_ceps]\n",
    "        peaks.append(peak)\n",
    "    return  f0,ceps,peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dtw(src,trg):\n",
    "    _, path = fastdtw(src, trg, dist=euclidean)\n",
    "    twf = np.array(path).T\n",
    "    return twf[0],twf[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mfcc(mel):\n",
    "    nceps=12\n",
    "    mel=mel.transpose(0,1)\n",
    "    for i, mspec in enumerate(mel):\n",
    "        #メルケプストラムを離散コサイン変換　係数は12次まで取る\n",
    "        ceps = fftpack.realtransforms.dct(mspec.float().data.cpu().numpy(), type=2, norm=\"ortho\", axis=-1)[np.newaxis,:]\n",
    "        if i == 0:\n",
    "            mfcc = ceps\n",
    "        else:\n",
    "            mfcc =np.concatenate([mfcc,ceps[:nceps]],0)\n",
    "    return torch.from_numpy(np.array(mfcc).astype(np.float32)).transpose(1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_mcd(cvt_mcc_dtw,trg_mcc_dtw):\n",
    "    diff2sum = np.sum((cvt_mcc_dtw - trg_mcc_dtw)**2, 1)\n",
    "    mcd = np.mean(10.0 / np.log(10.0) * np.sqrt(2 * diff2sum), 0)\n",
    "    return mcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conpute_F0_RMSE(list_p,list_g):\n",
    "    loss=[]\n",
    "    for p , g in zip( list_p,list_g):\n",
    "        #発話ごとに差の2乗和を計算\n",
    "        loss.append(1200*np.sqrt(sum(torch.pow((torch.log2(g)-torch.log2(p)),2))/p.size(0)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = './DATA/003'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#標本化周波数\n",
    "fs =16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "conpute = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$F_{0}RMSE = 1200\\sqrt{\\frac{1}{N_{v}}\\sum_{n=1}^{N_{v}}}(log_{2}f_{n}-log_{2}f'_{n})^2$[cent]\\\n",
    "$N_{v}$:有声フレーム数\\\n",
    "f:有声区間のf0\n",
    "\n",
    "#### 処理手順\n",
    "        1.MFCCを基にDTWにより対応付けられたフレームのペアを獲得する。\n",
    "        2.対応づけられたフレームのペアより両者が有声となるフレームを獲得する。\n",
    "        3.対応づけられたF0系列から有声区間を取り出す。\n",
    "        4.歪みを求める。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 教師データ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(join(base_dir,'tool/source_list'),'r')as f:\n",
    "    yangpaths =[]\n",
    "    for line in f:\n",
    "        if   line.split('/') [-3] == 'WAV'  :\n",
    "            yangpaths.append(line.replace('\\n','').replace('WAV','yangsaf').replace('.wav','.mat'))\n",
    "        else:\n",
    "            yangpaths.append(line.replace('\\n','').replace('WAV_df_fps','yangsaf').replace('.wav','.mat') )\n",
    "with open(join(base_dir,'tool/source_list'),'r')as f:\n",
    "    wavpaths =  [path.replace('\\n','')  for path in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:42,  2.33it/s]\n"
     ]
    }
   ],
   "source": [
    "if conpute:\n",
    "    gt_f0s = []\n",
    "    gt_vuvs = [] \n",
    "    gt_mels = []  \n",
    "    \n",
    "    for yangpath ,wavpath in tqdm( zip(yangpaths,wavpaths)):\n",
    "        mat = loadmat(yangpath)\n",
    "        f0= mat['source']['f0'][0,0]\n",
    "        vuv = mat['source']['vuv'][0,0]\n",
    "        f0=f0.astype(np.float32)\n",
    "        \n",
    "        vuv=vuv.astype(np.float32)\n",
    "\n",
    "        data,sr = load_wav_to_torch(wavpath)\n",
    "        mel_spec = compute_melspec(data)\n",
    "        if mel_spec.size(1) != f0.size:\n",
    "            f0 = f0[(2.5*np.arange(0,mel_spec.size(1)-1)).astype(np.int)]\n",
    "            f0 = np.append(f0,f0[-1])\n",
    "            vuv = vuv[(2.5*np.arange(0,mel_spec.size(1)-1)).astype(np.int)]\n",
    "            vuv = np.append(vuv,vuv[-1])\n",
    "        f0 = torch.from_numpy(f0)\n",
    "        vuv = torch.from_numpy(vuv)\n",
    "        \n",
    "        gt_f0s.append(f0)\n",
    "        gt_vuvs.append(vuv)\n",
    "        gt_mels.append(mel_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tacotron-F0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tacotron-F0より得られたF0と真値よりRMSEを計算する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 出力されたVUV,F0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_f0_path = glob(join(base_dir,'Tacotronf0_130ksteps/lf0s_and_vuvs','*pickle'))\n",
    "tf_f0_path.sort()\n",
    "\n",
    "tf_mel_path = glob((join(base_dir,'Tacotronf0_130ksteps/mels','*npy')))\n",
    "tf_mel_path.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:05, 18.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中央値:330.4582214355469[cent]\n"
     ]
    }
   ],
   "source": [
    "if conpute:\n",
    "    gt_dtws_f=[]\n",
    "    tf_dtws=[]\n",
    "    for f0_path,mel_path,gt_f0,gt_vuv,gt_mel in tqdm(zip(tf_f0_path,tf_mel_path,gt_f0s,gt_vuvs,gt_mels)):\n",
    "        #アライメント処理\n",
    "        #mfcc\n",
    "        mel = np.load(mel_path)\n",
    "        mel = torch.from_numpy(np.array(mel).astype(np.float32)).clone()\n",
    "        mfcc = get_mfcc(mel).transpose(0,1)\n",
    "        mfccg = get_mfcc(gt_mel).transpose(0,1) \n",
    "        \n",
    "        #DTW\n",
    "        twf_src,twf_trg = get_dtw(mfcc,mfccg)\n",
    "\n",
    "        #推定されたF0とVUV\n",
    "        lf0, vuv = load_lf0_vuv(f0_path)\n",
    "        f0 = torch.from_numpy(np.exp(lf0[twf_src]))\n",
    "        vuv = torch.from_numpy(vuv[twf_src])\n",
    "        \n",
    "        #リファレンス\n",
    "        gt_f0 = gt_f0[twf_trg]\n",
    "        gt_vuv = gt_vuv[twf_trg]\n",
    "        \n",
    "        #どちらも有声であるフレームを獲得する\n",
    "        v_index = [  i  for i, pare in enumerate (zip(vuv,gt_vuv)) if pare[0]==pare[1] and pare[0]==1]\n",
    "        #有声区間のみにする。\n",
    "        f0= f0[v_index]\n",
    "        gt_f0 = gt_f0[v_index]\n",
    "        \n",
    "        gt_dtws_f.append(gt_f0)\n",
    "        tf_dtws.append(f0)\n",
    "        \n",
    "    tf_F0_RMSE= conpute_F0_RMSE(tf_dtws,gt_dtws_f)\n",
    "    print('中央値:{}[cent]'.format(statistics.median(tf_F0_RMSE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### スペクトログラムより得られたF0 vs yangsaf GT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/Users/hironishi/.conda/envs/tacotron/lib/python3.6/site-packages/ipykernel_launcher.py:20: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "100it [00:38,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中央値:351.3866329074723[cent]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if conpute:\n",
    "    gt_dtws_fm=[]\n",
    "    tfm_dtws=[]\n",
    "    \n",
    "    for mel_path,gt_mel,gt_f0,gt_vuv in tqdm(zip(tf_mel_path,gt_mels,gt_f0s,gt_vuvs)):\n",
    "        \n",
    "        #mfcc\n",
    "        mel = np.load(mel_path)\n",
    "        mel = torch.from_numpy(np.array(mel).astype(np.float32)).clone()\n",
    "        mfcc = get_mfcc(mel).transpose(0,1)\n",
    "        mfccg = get_mfcc(gt_mel).transpose(0,1) \n",
    "        \n",
    "        #DTW\n",
    "        twf_src,twf_trg = get_dtw(mfcc,mfccg)\n",
    "        \n",
    "        #推定されたF0とVUV\n",
    "        spec = get_spec(mel.unsqueeze(0))\n",
    "        f0,_,peaks = conpute_f0_from_spec(spec)\n",
    "        f0=signal.medfilt(f0,5)\n",
    "        peaks = torch.from_numpy(np.array(peaks).astype(np.float32)).clone()\n",
    "        vuv=np.array([1.0 if peak>=0.01 else 0.0 for peak in peaks])\n",
    "        f0 = torch.from_numpy(f0[twf_src])\n",
    "        vuv = torch.from_numpy(vuv[twf_src])\n",
    "            \n",
    "        #リファレンス\n",
    "        gt_f0 = gt_f0[twf_trg]\n",
    "        gt_vuv = gt_vuv[twf_trg]    \n",
    "        \n",
    "        #どちらも有声であるフレームを獲得する\n",
    "        v_index = [  i  for i, pare in enumerate (zip(vuv,gt_vuv)) if pare[0]==pare[1] and pare[0]==1]\n",
    "       \n",
    "        #有声区間のみにする。\n",
    "        f0= f0[v_index]\n",
    "        gt_f0 = gt_f0[v_index]\n",
    "        \n",
    "        gt_dtws_fm.append(gt_f0)\n",
    "        tfm_dtws.append(f0)\n",
    "        \n",
    "            \n",
    "    tfm_F0_RMSE = conpute_F0_RMSE(tfm_dtws,gt_dtws_fm)\n",
    "    print('中央値:{}[cent]'.format(statistics.median(tfm_F0_RMSE)))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### スペクトログラムより得られたF0 vs GT_mel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/Users/hironishi/.conda/envs/tacotron/lib/python3.6/site-packages/ipykernel_launcher.py:20: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hironishi/.conda/envs/tacotron/lib/python3.6/site-packages/ipykernel_launcher.py:29: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "100it [01:12,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中央値:467.3875919078149[cent]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if conpute:\n",
    "    gt_dtws_fm_2=[]\n",
    "    tfm_dtws_2=[]\n",
    "    \n",
    "    for mel_path,gt_mel in tqdm(zip(tf_mel_path,gt_mels)):\n",
    "        \n",
    "        #mfcc\n",
    "        mel = np.load(mel_path)\n",
    "        mel = torch.from_numpy(np.array(mel).astype(np.float32)).clone()\n",
    "        mfcc = get_mfcc(mel).transpose(0,1)\n",
    "        mfccg = get_mfcc(gt_mel).transpose(0,1) \n",
    "        \n",
    "        #DTW\n",
    "        twf_src,twf_trg = get_dtw(mfcc,mfccg)\n",
    "        \n",
    "        #推定されたF0とVUV\n",
    "        spec = get_spec(mel.unsqueeze(0))\n",
    "        f0,_,peaks = conpute_f0_from_spec(spec)\n",
    "        f0=signal.medfilt(f0,5)\n",
    "        peaks = torch.from_numpy(np.array(peaks).astype(np.float32)).clone()\n",
    "        vuv=np.array([1.0 if peak>=0.01 else 0.0 for peak in peaks])\n",
    "        f0 = torch.from_numpy(f0[twf_src])\n",
    "        vuv = torch.from_numpy(vuv[twf_src])\n",
    "            \n",
    "        #リファレンス\n",
    "        spec_g = get_spec(gt_mel.unsqueeze(0))\n",
    "        f0_g,_,peaks_g = conpute_f0_from_spec(spec_g)\n",
    "        f0_g=signal.medfilt(f0_g,5)\n",
    "        peaks_g = torch.from_numpy(np.array(peaks_g).astype(np.float32)).clone()\n",
    "        vuv_g=np.array([1.0 if peak>=0.01 else 0.0 for peak in peaks_g])\n",
    "        gt_f0 = torch.from_numpy(f0_g[twf_trg])\n",
    "        gt_vuv = torch.from_numpy(vuv_g[twf_trg])\n",
    "        \n",
    "        #どちらも有声であるフレームを獲得する\n",
    "        v_index = [  i  for i, pare in enumerate (zip(vuv,gt_vuv)) if pare[0]==pare[1] and pare[0]==1]\n",
    "       \n",
    "        #有声区間のみにする。\n",
    "        f0= f0[v_index]\n",
    "        gt_f0 = gt_f0[v_index]\n",
    "        \n",
    "        gt_dtws_fm_2.append(gt_f0)\n",
    "        tfm_dtws_2.append(f0)\n",
    "        \n",
    "            \n",
    "    tfm_F0_RMSE_2 = conpute_F0_RMSE(tfm_dtws_2,gt_dtws_fm_2)\n",
    "    print('中央値:{}[cent]'.format(statistics.median(tfm_F0_RMSE_2)))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tacotron2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tacotron2では基本周波数は出力されないため、ケプストラム法よりスペクトログラムからF0を推定する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2_mel_path = glob((join(base_dir,'Tacotron2_227ksteps/mels','*npy')))\n",
    "t2_mel_path.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/Users/hironishi/.conda/envs/tacotron/lib/python3.6/site-packages/ipykernel_launcher.py:20: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "100it [00:38,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中央値:375.760827168634[cent]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if conpute:\n",
    "    gt_dtws_2=[]\n",
    "    t2_dtws=[]\n",
    "    \n",
    "    for mel_path,gt_mel,gt_f0,gt_vuv in tqdm(zip(t2_mel_path,gt_mels,gt_f0s,gt_vuvs)):\n",
    "        \n",
    "        #mfcc\n",
    "        mel = np.load(mel_path)\n",
    "        mel = torch.from_numpy(np.array(mel).astype(np.float32)).clone()\n",
    "        mfcc = get_mfcc(mel).transpose(0,1)\n",
    "        mfccg = get_mfcc(gt_mel).transpose(0,1) \n",
    "        \n",
    "        #DTW\n",
    "        twf_src,twf_trg = get_dtw(mfcc,mfccg)\n",
    "        \n",
    "        #推定されたF0とVUV\n",
    "        spec = get_spec(mel.unsqueeze(0))\n",
    "        f0,_,peaks = conpute_f0_from_spec(spec)\n",
    "        f0=signal.medfilt(f0,5)\n",
    "        peaks = torch.from_numpy(np.array(peaks).astype(np.float32)).clone()\n",
    "        vuv=np.array([1.0 if peak>=0.01 else 0.0 for peak in peaks])\n",
    "        f0 = torch.from_numpy(f0[twf_src])\n",
    "        vuv = torch.from_numpy(vuv[twf_src])\n",
    "            \n",
    "        #リファレンス\n",
    "        gt_f0 = gt_f0[twf_trg]\n",
    "        gt_vuv = gt_vuv[twf_trg]    \n",
    "        \n",
    "        #どちらも有声であるフレームを獲得する\n",
    "        v_index = [  i  for i, pare in enumerate (zip(vuv,gt_vuv)) if pare[0]==pare[1] and pare[0]==1]\n",
    "        \n",
    "        #有声区間のみにする。\n",
    "        f0= f0[v_index]\n",
    "        gt_f0 = gt_f0[v_index]\n",
    "        \n",
    "        gt_dtws_2.append(gt_f0)\n",
    "        t2_dtws.append(f0)\n",
    "        \n",
    "            \n",
    "    t2_F0_RMSE = conpute_F0_RMSE(t2_dtws,gt_dtws_2)\n",
    "    print('中央値:{}[cent]'.format(statistics.median(t2_F0_RMSE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/Users/hironishi/.conda/envs/tacotron/lib/python3.6/site-packages/ipykernel_launcher.py:20: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hironishi/.conda/envs/tacotron/lib/python3.6/site-packages/ipykernel_launcher.py:30: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "100it [01:13,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中央値:489.1543615556982[cent]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if conpute:\n",
    "    gt_dtws_22=[]\n",
    "    t2_dtws_2=[]\n",
    "    \n",
    "    for mel_path,gt_mel in tqdm(zip(t2_mel_path,gt_mels)):\n",
    "        \n",
    "        #mfcc\n",
    "        mel = np.load(mel_path)\n",
    "        mel = torch.from_numpy(np.array(mel).astype(np.float32)).clone()\n",
    "        mfcc = get_mfcc(mel).transpose(0,1)\n",
    "        mfccg = get_mfcc(gt_mel).transpose(0,1) \n",
    "        \n",
    "        #DTW\n",
    "        twf_src,twf_trg = get_dtw(mfcc,mfccg)\n",
    "        \n",
    "        #推定されたF0とVUV\n",
    "        spec = get_spec(mel.unsqueeze(0))\n",
    "        f0,_,peaks = conpute_f0_from_spec(spec)\n",
    "        f0=signal.medfilt(f0,5)\n",
    "        peaks = torch.from_numpy(np.array(peaks).astype(np.float32)).clone()\n",
    "        vuv=np.array([1.0 if peak>=0.01 else 0.0 for peak in peaks])\n",
    "        f0 = torch.from_numpy(f0[twf_src])\n",
    "        vuv = torch.from_numpy(vuv[twf_src])\n",
    "            \n",
    "        #リファレンス\n",
    "        spec_g = get_spec(gt_mel.unsqueeze(0))\n",
    "        \n",
    "        f0_g,_,peaks_g = conpute_f0_from_spec(spec_g)\n",
    "        f0_g=signal.medfilt(f0_g,5)\n",
    "        peaks_g = torch.from_numpy(np.array(peaks_g).astype(np.float32)).clone()\n",
    "        vuv_g=np.array([1.0 if peak>=0.01 else 0.0 for peak in peaks_g])\n",
    "        gt_f0 = torch.from_numpy(f0_g[twf_trg])\n",
    "        gt_vuv = torch.from_numpy(vuv_g[twf_trg])\n",
    "    \n",
    "        #どちらも有声であるフレームを獲得する\n",
    "        v_index = [  i  for i, pare in enumerate (zip(vuv,gt_vuv)) if pare[0]==pare[1] and pare[0]==1]\n",
    "        \n",
    "        #有声区間のみにする。\n",
    "        f0= f0[v_index]\n",
    "        gt_f0 = gt_f0[v_index]\n",
    "        \n",
    "        gt_dtws_22.append(gt_f0)\n",
    "        t2_dtws_2.append(f0)\n",
    "        \n",
    "            \n",
    "    t2_F0_RMSE_2 = conpute_F0_RMSE(t2_dtws_2,gt_dtws_22)\n",
    "    print('中央値:{}[cent]'.format(statistics.median(t2_F0_RMSE_2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 調査に使用した評価音声の条件 003\n",
    "各性別5名,言い淀みとフィラーを含まない100音声\n",
    "\n",
    "#### TacotronF0 130kstep学習\n",
    "#### 出力されたF0より求めた場合\n",
    "中央値:330.4582214355469[cent]\n",
    "#### F0&vuvをケプストラムより求めた場合\n",
    "中央値:351.3866329074723[cent]\n",
    "#### Tacotron2 130kstep学習\n",
    "中央値:367.78043262805863[cent]\n",
    "#### Tacotron2 227kstep学習\n",
    "中央値:375.760827168634[cent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### 調査に使用した評価音声の条件 002\n",
    "各性別50名,言い淀みとフィラーを含む：含まない=1:1の100音声\n",
    "\n",
    "#### TacotronF0 13kstep学習\n",
    "#### 出力されたF0より求めた場合\n",
    "中央値:340.4158020019531[cent]\n",
    "#### F0&vuvをケプストラムより求めた場合\n",
    "中央値:358.41509034067303[cent]\n",
    "#### Tacotron2 227kstep学習\n",
    "中央値:431.6830528356909[cent]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conpute:\n",
    "    labels=['TacotronF0(1)','TacotronF0(2)','Tacotron2']\n",
    "    plt.hist([tf_F0_RMSE,tfm_F0_RMSE,t2_F0_RMSE],bins=15,label=labels)\n",
    "    plt.xlabel('F0[cent]')\n",
    "    plt.legend()\n",
    "    #plt.savefig('F0-RMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(gt_dtws_2[21])\n",
    "plt.plot(t2_dtws[21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(gt_dtws_f[21])\n",
    "plt.plot(tf_dtws[21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(gt_dtws_fm[21])\n",
    "plt.plot(tfm_dtws[21])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "検定は\n",
    "\n",
    "    ・等分散の検定\n",
    "    ・ガンマ分布\n",
    "\n",
    "のどちらか\n",
    "今回は等分散の検定で行う。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$H_0$ :両モデルの分散は等しい,$H_1$ :両モデルの分散は異なる\n",
    "検定にはscipy.stats.bartlett()をもちいる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "検定の結果、p値が$0.173$であり0.05を上回るため$H_0$は棄却できない\\\n",
    "次に、2群間のt検定を行なった結果\\\n",
    "Ttest_indResult(statistic=5.981377666233164, pvalue=1.0182857916646957e-08)\\\n",
    "であり、有意に差があることがわかった。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F0とスペクトログラムの整合性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tacotron-F0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "conpute_1= True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conpute_1:\n",
    "    lf0s = []\n",
    "    lf0_ms = []\n",
    "    loss=[]\n",
    "    for f0_path,mel_path in tqdm(zip(tf_f0_path,tf_mel_path )):\n",
    "        lf0, vuv=load_lf0_vuv(f0_path)\n",
    "        lf0 = torch.from_numpy(np.log2(np.exp(lf0)))*torch.from_numpy(vuv)\n",
    "        \n",
    "        mel = np.load(mel_path)\n",
    "        mel =  torch.from_numpy(np.array(mel).astype(np.float32)).clone()\n",
    "        spec = get_spec(mel.unsqueeze(0))\n",
    "        f0_m,_ ,peaks= conpute_f0_from_spec(spec)\n",
    "        f0_m=signal.medfilt(f0_m,5)\n",
    "        #vuv_p= [1.0 if peak>=0.01  else 0.0 for peak in peaks]\n",
    "        lf0_m = np.log2(f0_m)\n",
    "        lf0_m = lf0_m*vuv\n",
    "        lf0_m = torch.from_numpy(np.array(lf0_m).astype(np.float32))\n",
    "        lf0s.append(lf0)\n",
    "        lf0_ms.append(lf0_m)\n",
    "        loss.append(1200*torch.sqrt(torch.nn.MSELoss()(lf0_m,lf0)))\n",
    "    print('中央値:{}cent'.format(statistics.median(loss)))    \n",
    "    plt.hist(loss,bins=100)\n",
    "    plt.xlabel('F0[cent]')\n",
    "    #plt.savefig('./Consistency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:34,  2.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中央値:171.808349609375cent\n",
      "有声無声一致率:72.64249148753363%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9a446843c8>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if conpute_1:\n",
    "    lf0sm_g = []\n",
    "    lf0sy_g = []\n",
    "    loss_g=[]\n",
    "    vuv_match_rates = []\n",
    "    for mel,f0,vuv in tqdm(zip(gt_mels,gt_f0s,gt_vuvs)):\n",
    "        \n",
    "        spec = get_spec(mel.unsqueeze(0))\n",
    "        f0_m,_ ,peaks= conpute_f0_from_spec(spec)\n",
    "        f0_m=signal.medfilt(f0_m,5)\n",
    "        vuv_m=torch.from_numpy(np.array( [1.0 if peak>=0.01  else 0.0 for peak in peaks]).astype(np.float32))\n",
    "        lf0_m = np.log2(f0_m)\n",
    "        lf0_m = torch.from_numpy(np.array(lf0_m).astype(np.float32))\n",
    "        lf0_mv = lf0_m*vuv\n",
    "        lf0_mvm = lf0_m*vuv_m\n",
    "        \n",
    "        lf0 = torch.log2(f0)*vuv\n",
    "        vuv_match_rate = sum([   1 if v==vm else 0 for v,vm in zip(vuv,vuv_m)])/vuv.size(0)*100\n",
    "        vuv_match_rates.append(vuv_match_rate )\n",
    "        \n",
    "        lf0sy_g.append(lf0)\n",
    "        lf0sm_g.append(lf0_mvm)\n",
    "        \n",
    "        loss_g.append(1200*torch.sqrt(torch.nn.MSELoss()(lf0_mv,lf0)))\n",
    "    \n",
    "    print('中央値:{}cent'.format(statistics.median(loss_g)))\n",
    "    print('有声無声一致率:{}%'.format(np.mean(vuv_match_rates)))\n",
    "    plt.hist(loss_g,bins=100)\n",
    "    plt.xlabel('F0[cent]')\n",
    "    #plt.savefig('./Consistency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lf0sm_g[4])\n",
    "plt.plot(lf0sy_g[4]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel=np.load(t2_mel_path[71])\n",
    "mel =  torch.from_numpy(np.array(mel).astype(np.float32)).clone()\n",
    "spec = get_spec(mel.unsqueeze(0))\n",
    "data,sr = load_wav_to_torch(wavpaths[71])\n",
    "mel_g= compute_melspec(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f0,ceps,peaks = conpute_f0_from_spec(spec) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#melからmfccを得る \n",
    "mfcc = get_mfcc(mel)\n",
    "mfcc_g = get_mfcc(mel_g)\n",
    "# DTWを行い配列を得る。\n",
    "twf = get_dtw(mfcc_g.transpose(0,1),mfcc.transpose(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f0,ceps,peaks = conpute_f0_from_spec(spec)\n",
    "f0= signal.medfilt(f0,5)\n",
    "f0=np.array(f0)\n",
    "\n",
    "mat = loadmat(yangpaths[71])\n",
    "f0_g= mat['source']['f0'][0,0]\n",
    "vuv_g =mat['source']['vuv'][0,0]\n",
    "f0_g=f0_g.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vuv = np.array([ x[0]  for x in vuv_g])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(f0[twf[1]]*vuv[twf[0]])\n",
    "plt.plot(f0_g[twf[0]]*vuv_g[twf[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f0,ceps,peaks = conpute_f0_from_spec(spec)\n",
    "vuv_p= [1.0 if peak>=0.01  else 0.0 for peak in peaks]\n",
    "f0= signal.medfilt(f0,5)\n",
    "f0 = f0*np.array(vuv_p)\n",
    "lf0, vuv=load_lf0_vuv(tf_f0_path[71])\n",
    "plt.plot(np.exp(lf0)*vuv)\n",
    "plt.plot(f0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分析再合成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conpute and syn:\n",
    "    from audio_processing import griffin_lim\n",
    "    from scipy.io.wavfile  import write\n",
    "    out_dir = join(base_dir,'Grand_truth_griffin_lim')\n",
    "    os.makedirs(out_dir,exist_ok=True)\n",
    "    stft = layers.TacotronSTFT(1024,200,800, sampling_rate=16000)\n",
    "    for i, mel in enumerate(gt_mels):\n",
    "        spec = get_spec(mel.unsqueeze(0))\n",
    "        audio = griffin_lim(torch.autograd.Variable(spec[:, :, :-1]), stft.stft_fn, 100)\n",
    "        write(join(out_dir,'text_{}.wav'.format(str(i).zfill(3))),16000,audio[0].data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
